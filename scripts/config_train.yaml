# TODO: Maybe set up Paths, etc for this
config:
  smoke_test: true
  checkpoints_dir: "/net/projects/cgfp/checkpoints/"
data:
  text_field: "Product Type"
  data_dir: "/net/projects/cgfp/data/"
  train_filename: "clean_CONFIDENTIAL_CGFP_bulk_data_073123.csv"
  eval_filename: "combined_eval_set.csv"
  test_filename: "TestData_11.22.23.xlsx"
model:
  model_name: "distilbert" # Choices: distilbert, roberta
  starting_checkpoint: null
  # starting_checkpoint: "/net/projects/cgfp/model-files/distilbert-base-uncased_20240723_1230_update_config_counts" # null to train the off-the-shelf huggingface model
  eval_checkpoint: "/net/projects/cgfp/model-files/distilbert-base-uncased_20240723_1230_update_config_counts"
  model_dir: "/net/projects/cgfp/model-files/"
  classification_head_type: "mlp" # Choices: "mlp", "linear"
  loss: "cross_entropy" # Choices: "cross_entropy"
  save_best: true # Model saving strategy. If set to false, saves last model.
  freeze_base: true # Freezes the base model and only trains the classification heads
  reset_classification_heads: false # Renitializes the classification heads
  attached_heads: null # If null, attaches all heads to computation graph. If not null, attaches only the specified heads
    # - "Basic Type"
    # - "Sub-Types"
  combine_subtypes: true # Combine sub-type columns for multi-label training
training:
  lr: 0.001
  epochs: 1 # Recommended: 80 for full model training, 20 for classification head training
  train_batch_size: 32
  eval_batch_size: 64
  metric_for_best_model: "mean_f1_score" # Choices: "mean_f1_score", "basic_type_accuracy"
  eval_prompt: "frozen peas and carrots"
adamw:
  betas:
    - 0.9
    - 0.95
  eps: 1e-5 # epsilon, prevents division by zero
  weight_decay: 0.01
scheduler: # CosineAnnealingWarmRestarts
  T_0: 2000 # Number of iterations before first restart
  T_mult: 1 # Multiplicative factor by which to increase T after each restart
  eta_min_constant: .1 # Minimum learning rate constant