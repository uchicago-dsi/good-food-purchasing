config:
  smoke_test: true
  checkpoints_dir: "/net/projects/cgfp/checkpoints/"
  run_title: "final_all_heads_full_sft"
data:
  text_field: "Product Type"
  data_dir: "/net/projects/cgfp/data/"
  train_filenames: # Note: takes lists or single filenames
    - "clean_CONFIDENTIAL_CGFP_bulk_data_073123.csv" 
    - "clean_Non-food_Items_082024.csv"
  eval_filenames:  # Note: takes lists or single filenames
    - "clean_TestData_11.22.23.csv"
    - "clean_PPS_new_dataset.csv"
    - "clean_NYCDHS_DSI_Data.csv"
  test_filename: "TestData_11.22.23.xlsx"  # This should just be a single filename
model:
  model_name: "roberta" # Choices: distilbert, roberta
  # starting_checkpoint: "/net/projects/cgfp/huggingface/cgfp-roberta" # null to train the off-the-shelf huggingface model
  starting_checkpoint: "/net/projects/cgfp/model-files/roberta_20240910_1156_final_subtypes_full_sft" 
  model_dir: "/net/projects/cgfp/model-files/"
  classification_head_type: "mlp" # Choices: "mlp", "linear"
  loss: "cross_entropy" # Choices: "cross_entropy"
  save_best: true # Model saving strategy. If set to false, saves last model.
  freeze_base: false # Freezes the base model and only trains the classification heads
  reset_classification_heads: false # Renitializes the classification heads
  attached_heads: null # If null, attaches all heads to computation graph. If not null, attaches only the specified heads
    # - "Sub-Types"
  update_config: true # Update the model config with new counts, etc. from data (set to false to prevent smoke test from updating)
training:
  lr: 2e-5 # .001 for distilbert, 2e-5 for roberta
  epochs: 30 # Recommended: 80 for full model training, 20 for classification head training
  train_batch_size: 32
  eval_batch_size: 64
  metric_for_best_model: "mean_f1_score" # Choices: "mean_f1_score", "basic_type_accuracy"
  eval_prompt: "frozen peas and carrots"
adamw:
  betas:
    - 0.9
    - 0.95
  eps: 1e-5 # epsilon, prevents division by zero
  weight_decay: 0.01
scheduler: # CosineAnnealingWarmRestarts
  T_0: 2000 # Number of iterations before first restart
  T_mult: 1 # Multiplicative factor by which to increase T after each restart
  eta_min_constant: .1 # Minimum learning rate constant
eval:
  eval_checkpoint: "uchicago-dsi/cgfp-roberta"
