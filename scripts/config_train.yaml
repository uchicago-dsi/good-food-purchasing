# TODO: Maybe set up Paths, etc for this
config:
  smoke_test: true
  checkpoints_dir: "/net/projects/cgfp/checkpoints/"
  run_title: "test_new_data"
data:
  text_field: "Product Type"
  data_dir: "/net/projects/cgfp/data/"
  train_filenames: "clean_CONFIDENTIAL_CGFP_bulk_data_073123.csv"  # Note: takes lists or single filenames
  eval_filenames: "combined_eval_set.csv"  # Note: takes lists or single filenames
  test_filename: "TestData_11.22.23.xlsx"  # This should just be a single filename
model:
  model_name: "roberta" # Choices: distilbert, roberta
  starting_checkpoint: "/net/projects/cgfp/huggingface/cgfp-roberta"
  # starting_checkpoint: "/net/projects/cgfp/model-files/distilbert_20240813_1603_raw_distilbert" # null to train the off-the-shelf huggingface model
  eval_checkpoint: "/net/projects/cgfp/model-files/roberta_20240726_0823_heads_only_uncased"
  model_dir: "/net/projects/cgfp/model-files/"
  classification_head_type: "mlp" # Choices: "mlp", "linear"
  loss: "cross_entropy" # Choices: "cross_entropy"
  save_best: true # Model saving strategy. If set to false, saves last model.
  freeze_base: true # Freezes the base model and only trains the classification heads
  reset_classification_heads: true # Renitializes the classification heads
  attached_heads: null # If null, attaches all heads to computation graph. If not null, attaches only the specified heads
    # - "Basic Type"
  combine_subtypes: true # Combine sub-type columns for multi-label training
  update_config: true # Update the model config with new counts, etc. from data (set to false to prevent smoke test from updating)
training:
  lr: 2e-5 # .001 for distilbert, 2e-5 for roberta
  epochs: 20 # Recommended: 80 for full model training, 20 for classification head training
  train_batch_size: 32
  eval_batch_size: 64
  metric_for_best_model: "mean_f1_score" # Choices: "mean_f1_score", "basic_type_accuracy"
  eval_prompt: "frozen peas and carrots"
adamw:
  betas:
    - 0.9
    - 0.95
  eps: 1e-5 # epsilon, prevents division by zero
  weight_decay: 0.01
scheduler: # CosineAnnealingWarmRestarts
  T_0: 2000 # Number of iterations before first restart
  T_mult: 1 # Multiplicative factor by which to increase T after each restart
  eta_min_constant: .1 # Minimum learning rate constant